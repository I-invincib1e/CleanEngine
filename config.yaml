# Advanced Dataset Cleaner Configuration
# This file allows you to customize default settings for data cleaning and analysis

# Data Cleaning Settings
cleaning:
  missing_values:
    threshold: 0.5              # Drop columns with >50% missing values
    strategy: "auto"            # auto, mean, median, mode, drop
    fill_numeric: "median"      # mean, median, mode
    fill_categorical: "mode"    # mode, constant
    
  duplicates:
    remove: true               # Remove duplicate rows
    keep: "first"              # first, last, false
    
  outliers:
    method: "iqr"              # iqr, zscore, isolation_forest
    threshold: 1.5             # IQR multiplier or Z-score threshold
    action: "remove"           # remove, cap, flag
    
  encoding:
    categorical_method: "label"  # label, onehot, target
    handle_unknown: "ignore"     # ignore, error
    
  normalization:
    method: "minmax"           # minmax, standard, robust
    feature_range: [0, 1]      # For MinMax scaling

# Advanced Analysis Settings  
analysis:
  enable: true                 # Enable advanced analysis
  
  statistical:
    enable: true
    include_skewness: true
    include_kurtosis: true
    normality_test: true
    
  correlation:
    enable: true
    method: "pearson"          # pearson, spearman, kendall
    threshold: 0.7             # Strong correlation threshold
    
  clustering:
    enable: true
    method: "kmeans"           # kmeans, hierarchical, dbscan
    max_clusters: 10           # Maximum clusters to consider
    
  anomaly_detection:
    enable: true
    method: "isolation_forest" # isolation_forest, local_outlier_factor
    contamination: 0.1         # Expected proportion of outliers
    
  feature_importance:
    enable: true
    method: "mutual_info"      # mutual_info, correlation, variance
    
  quality_assessment:
    enable: true
    completeness_weight: 0.4   # Weight for completeness in quality score
    uniqueness_weight: 0.3     # Weight for uniqueness in quality score
    consistency_weight: 0.3    # Weight for consistency in quality score

# Visualization Settings
visualization:
  enable: true
  style: "seaborn-v0_8"       # matplotlib style
  color_palette: "viridis"    # Color palette for plots
  figure_size: [12, 8]        # Default figure size
  dpi: 300                    # Resolution for saved plots
  format: "png"               # png, jpg, pdf, svg

# Output Settings
output:
  create_folders: true        # Create organized output folders
  folder_prefix: "Cleans-"    # Prefix for output folders
  save_original: false        # Save copy of original data
  compress_output: false      # Compress output files
  
  reports:
    generate_markdown: true   # Generate markdown reports
    generate_json: true       # Generate JSON reports
    generate_html: false      # Generate HTML reports
    include_visualizations: true

# Performance Settings
performance:
  chunk_size: 10000          # Process data in chunks for large datasets
  parallel_processing: false # Enable parallel processing (experimental)
  memory_limit_gb: 4         # Memory limit for processing
  
# Logging Settings
logging:
  level: "INFO"              # DEBUG, INFO, WARNING, ERROR, CRITICAL
  file: "dataset_cleaner.log" # Log file name
  max_size_mb: 10            # Maximum log file size
  backup_count: 3            # Number of backup log files
  
# File Format Settings
file_formats:
  csv:
    encoding: "auto"         # auto, utf-8, latin-1, etc.
    delimiter: ","           # CSV delimiter
    
  excel:
    engine: "openpyxl"       # openpyxl, xlrd
    
  json:
    orient: "records"        # records, index, values, split
    
  parquet:
    engine: "pyarrow"        # pyarrow, fastparquet

# Validation Rules (Custom)
validation:
  enable: false              # Enable custom validation rules
  rules: []                  # Custom validation rules (to be defined)
  
# Advanced Features (Experimental)
experimental:
  time_series_analysis: false
  text_analysis: false
  geospatial_analysis: false
  ml_recommendations: false