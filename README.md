# ğŸ§¹ CleanEngine

[![Tests](https://img.shields.io/badge/tests-36%20passing-brightgreen)](./scripts/run_tests.py)
[![Python](https://img.shields.io/badge/python-3.9%2B-blue)](#)
[![License: MIT](https://img.shields.io/badge/license-MIT-yellow)](./LICENSE)
[![CLI](https://img.shields.io/badge/CLI-Rich%20%2B%20Typer-purple)](#)
[![Status](https://img.shields.io/badge/status-production%20ready-success)](#)
[![Version](https://img.shields.io/badge/version-0.1.0-orange)](#)

> **ğŸš€ The Ultimate Data Cleaning & Analysis Toolkit**  
> Transform messy datasets into clean, insights-rich data with CleanEngine's powerful cleaning pipeline, intelligent analysis, and YAML-driven rule engine.

---

## ğŸŒŸ What Makes CleanEngine Special?

| Feature | **CleanEngine** ğŸ§¹ | pandas-profiling | Sweetviz | Great Expectations |
|---------|-------------------|------------------|-----------|-------------------|
| **Data Cleaning** | âœ… **Complete Pipeline** | âŒ No | âŒ No | âš ï¸ Limited |
| **Profiling & Stats** | âœ… **Advanced Analytics** | âœ… Yes | âœ… Yes | âš ï¸ Minimal |
| **Correlation Analysis** | âœ… **Multi-Method** | âœ… Yes | âœ… Yes | âŒ No |
| **Feature Importance** | âœ… **ML-Powered** | âŒ No | âŒ No | âŒ No |
| **Clustering & Patterns** | âœ… **3 Algorithms** | âŒ No | âŒ No | âŒ No |
| **Anomaly Detection** | âœ… **2 Methods** | âŒ No | âŒ No | âŒ No |
| **Rule Engine** | âœ… **YAML-Driven** | âŒ No | âŒ No | âœ… Yes |
| **Interfaces** | âœ… **CLI + GUI + Watcher** | CLI/Notebook | Notebook | CLI/Notebook |
| **Automation** | âœ… **Folder Watcher** | âŒ No | âŒ No | âœ… Yes |

---

## âœ¨ Core Features

### ğŸ§¼ **Smart Data Cleaning**
- **Missing Values**: Intelligent imputation strategies
- **Duplicate Detection**: Advanced duplicate removal with configurable thresholds
- **Outlier Handling**: IQR, Z-score, and custom outlier detection
- **Encoding**: Automatic categorical encoding and normalization
- **Data Types**: Smart type inference and conversion

### ğŸ“Š **Advanced Analytics**
- **Statistical Analysis**: Comprehensive descriptive statistics
- **Correlation Methods**: Pearson, Spearman, and Kendall correlations
- **Feature Importance**: Mutual information and target analysis
- **Distribution Analysis**: Skewness, kurtosis, and normality tests
- **Data Quality Scoring**: Completeness, uniqueness, and consistency metrics

### ğŸ” **Machine Learning Insights**
- **Clustering**: K-Means, DBSCAN, and Hierarchical clustering
- **Anomaly Detection**: Isolation Forest and Local Outlier Factor (LOF)
- **Pattern Recognition**: Automatic pattern discovery in data
- **Dimensionality Insights**: Feature correlation and redundancy analysis

### âš™ï¸ **Rule Engine & Validation**
- **YAML Configuration**: Declarative rule definitions
- **Pre/Post Validation**: Data quality checks before and after cleaning
- **Custom Rules**: Extensible rule system for domain-specific validation
- **Compliance**: Built-in support for data governance requirements

### ğŸ¨ **Multiple Interfaces**
- **Rich CLI**: Beautiful terminal interface with Typer and Rich
- **Streamlit GUI**: Interactive web-based interface
- **Folder Watcher**: Automated processing of new datasets
- **API Ready**: Modular design for integration

---

## ğŸš€ Quick Start

### ğŸ“‹ **Prerequisites**
- Python 3.9 or higher
- Git (for cloning)
- pip or conda package manager

### ğŸ”§ **Installation**
```bash
# 1. Clone the repository
git clone https://github.com/I-invincib1e/CleanEngine.git
cd CleanEngine

# 2. Install dependencies
python setup.py

# 3. Verify installation
python main.py -t
```

### ğŸ¯ **First Run**
```bash
# Interactive menu (recommended for beginners)
python main.py

# Quick commands
python main.py -s                  # ğŸ² Create sample datasets
python main.py -c sample_mixed.csv # ğŸ§¹ Clean a dataset
python main.py -t                  # ğŸ§ª Run tests
python main.py -g                  # ğŸŒ Launch Streamlit GUI
```

---

## ğŸš€ Minimal Example

```python
import dataset_cleaner
print(dataset_cleaner.__version__)

from dataset_cleaner.core.cleaner import DatasetCleaner
cleaner = DatasetCleaner()
cleaned_df = cleaner.clean_dataset('data.csv')
cleaner.create_output_folder('data.csv')
cleaner.save_results(cleaned_df, cleaner.report, 'Cleans-data')
```

---

## ğŸ® CLI Commands (Typer + Rich)

| Command | Short | Description | Example |
|---------|-------|-------------|---------|
| `clean` | `c` | Clean a dataset | `python main.py c data.csv` |
| `samples` | `s` | Generate sample data | `python main.py s` |
| `tests` | `t` | Run test suite | `python main.py t` |
| `gui` | `g` | Launch Streamlit | `python main.py g` |

### ğŸ¨ **CLI Features**
- **Rich Output**: Colorful tables, progress bars, and status indicators
- **Interactive Menus**: User-friendly selection interfaces
- **Short Flags**: Quick access with single-letter commands
- **Help System**: Comprehensive command documentation
- **Auto-completion**: Intelligent command suggestions

---

## âš™ï¸ Configuration & Rule Engine

### ğŸ“ **Configuration Structure**
```yaml
# config/default_config.yaml
cleaning:
  missing_values:
    strategy: "auto"  # auto, mean, median, mode, drop
  outliers:
    method: "iqr"     # iqr, zscore, custom
  encoding:
    categorical: true
    normalize: true

analysis:
  correlation:
    method: "pearson"  # pearson, spearman, kendall
  clustering:
    method: "kmeans"   # kmeans, dbscan, hierarchical
  anomaly_detection:
    method: "isolation_forest"  # isolation_forest, lof

validation:
  enable: true
  rules:
    - type: "column_exists"
      params: { column: "age" }
    - type: "dtype_in"
      params: { column: "age", dtypes: ["int64", "float64"] }
    - type: "max_missing_pct"
      params: { column: "income", threshold: 20 }
```

### ğŸ” **Available Validation Rules**
- **`column_exists`**: Verify required columns are present
- **`dtype_in`**: Check column data types
- **`max_missing_pct`**: Enforce maximum missing data percentage
- **`value_range`**: Validate numeric value ranges
- **`allowed_values`**: Restrict categorical values
- **`max_duplicates_pct`**: Control duplicate percentage
- **`custom_rule`**: User-defined validation logic

---

## ğŸ“Š **Analysis Capabilities**

### ğŸ“ˆ **Statistical Analysis**
- **Descriptive Statistics**: Mean, median, std, min, max, quartiles
- **Distribution Analysis**: Histograms, box plots, normality tests
- **Correlation Matrix**: Heatmaps with multiple correlation methods
- **Data Quality Metrics**: Completeness, uniqueness, consistency scores

### ğŸ¯ **Feature Analysis**
- **Importance Ranking**: Mutual information and correlation-based ranking
- **Target Analysis**: Feature relationships with target variables
- **Pattern Discovery**: Automatic pattern identification
- **Dimensionality Insights**: Feature correlation and redundancy

### ğŸ”¬ **Advanced Analytics**
- **Clustering Analysis**: 
  - **K-Means**: Centroid-based clustering with elbow method
  - **DBSCAN**: Density-based clustering for irregular shapes
  - **Hierarchical**: Tree-based clustering with dendrograms
- **Anomaly Detection**:
  - **Isolation Forest**: Fast anomaly detection for large datasets
  - **Local Outlier Factor**: Density-based outlier detection

### ğŸ“… **Time Series Analysis**
- **Trend Analysis**: Moving averages and trend detection
- **Seasonality**: Seasonal decomposition and pattern recognition
- **Forecasting**: Basic time series forecasting capabilities
- **Visualization**: Time series plots and analysis charts

---

## ğŸ¨ **Outputs & Reports**

### ğŸ“ **File Structure**
```
Cleans-<dataset_name>/
â”œâ”€â”€ cleaned_data.csv          # Cleaned dataset
â”œâ”€â”€ cleaning_report.json      # Detailed cleaning summary
â”œâ”€â”€ analysis_report.json      # Comprehensive analysis results
â”œâ”€â”€ visualizations/           # Generated charts and plots
â”‚   â”œâ”€â”€ correlation_heatmap.png
â”‚   â”œâ”€â”€ distribution_plots.png
â”‚   â”œâ”€â”€ clustering_results.png
â”‚   â””â”€â”€ anomaly_detection.png
â””â”€â”€ logs/                     # Processing logs
```

### ğŸ“Š **Report Contents**
- **Cleaning Summary**: Statistics on data transformations
- **Quality Metrics**: Before/after data quality scores
- **Validation Results**: Rule engine outcomes
- **Analysis Insights**: Statistical and ML analysis results
- **Recommendations**: Suggested next steps and improvements

---

## ğŸ—ï¸ **Project Architecture**

```
src/dataset_cleaner/
â”œâ”€â”€ core/
â”‚   â””â”€â”€ cleaner.py           # ğŸ§¹ Main cleaning pipeline
â”œâ”€â”€ analysis/
â”‚   â”œâ”€â”€ analyzer.py          # ğŸ“Š Analysis orchestrator
â”‚   â”œâ”€â”€ statistical_tests.py # ğŸ“ˆ Statistical testing
â”‚   â””â”€â”€ time_series.py       # ğŸ“… Time series analysis
â”œâ”€â”€ interfaces/
â”‚   â”œâ”€â”€ streamlit_app.py     # ğŸŒ Web GUI
â”‚   â””â”€â”€ folder_watcher.py    # ğŸ“ Automated processing
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ config_manager.py    # âš™ï¸ Configuration management
â”‚   â”œâ”€â”€ file_handler.py      # ğŸ“ File I/O operations
â”‚   â”œâ”€â”€ logger_setup.py      # ğŸ“ Logging configuration
â”‚   â””â”€â”€ rule_engine.py       # ğŸ” Validation rule engine
â””â”€â”€ __init__.py

scripts/                      # ğŸš€ Entry points
â”œâ”€â”€ run_cleaner.py           # CLI for cleaning
â”œâ”€â”€ run_tests.py             # Test runner
â””â”€â”€ create_sample_data.py    # Sample data generator

config/                       # âš™ï¸ Configuration files
â”œâ”€â”€ default_config.yaml      # Default settings
â””â”€â”€ config.yaml              # User overrides

tests/                        # ğŸ§ª Test suite
docs/                         # ğŸ“š Documentation
```

---

## ğŸ”§ **Advanced Usage**

### ğŸš€ **Batch Processing**
```bash
# Process multiple files
for file in *.csv; do
    python main.py -c "$file"
done
```

### ğŸ“ **Folder Watching**
```python
from src.dataset_cleaner.interfaces.folder_watcher import FolderWatcher

watcher = FolderWatcher(
    input_dir="./input",
    output_dir="./output",
    config_path="./config/config.yaml"
)
watcher.start()
```

### âš™ï¸ **Custom Configuration**
```python
from src.dataset_cleaner.utils.config_manager import ConfigManager

config = ConfigManager()
config.update({
    "cleaning": {
        "missing_values": {"strategy": "drop"},
        "outliers": {"method": "zscore", "threshold": 3}
    }
})
```

---

## ğŸ§ª **Testing & Quality**

### ğŸ§ª **Running Tests**
```bash
# All tests
python main.py -t

# Specific test file
python -m pytest tests/test_data_analyzer.py -v

# With coverage
python -m pytest --cov=src/dataset_cleaner tests/
```

### ğŸ“Š **Test Coverage**
- **Unit Tests**: Core functionality testing
- **Integration Tests**: End-to-end workflow testing
- **Performance Tests**: Large dataset handling
- **Edge Cases**: Error handling and boundary conditions

---

## ğŸ“š **Documentation & Community**

### ğŸ“– **Guides & References**
- **Contributing**: [CONTRIBUTING.md](./CONTRIBUTING.md) - Setup, code style, tests, PR rules
- **Code of Conduct**: [CODE_OF_CONDUCT.md](./CODE_OF_CONDUCT.md) - Community guidelines
- **Security**: [SECURITY.md](./SECURITY.md) - Vulnerability reporting
- **Code Owners**: [CODEOWNERS](./CODEOWNERS) - Review requirements

### ğŸ¤ **Getting Help**
- **Issues**: Report bugs and request features
- **Discussions**: Ask questions and share ideas
- **Wiki**: Community-contributed guides and tips
- **Examples**: Sample datasets and use cases

---

## ğŸš€ **Performance & Scalability**

### âš¡ **Optimization Features**
- **Memory Management**: Efficient handling of large datasets
- **Parallel Processing**: Multi-threaded operations where possible
- **Lazy Evaluation**: On-demand computation for large operations
- **Caching**: Intelligent caching of intermediate results

### ğŸ“Š **Performance Benchmarks**
- **Small Datasets** (< 1MB): < 1 second
- **Medium Datasets** (1-100MB): 1-30 seconds
- **Large Datasets** (100MB-1GB): 30 seconds - 5 minutes
- **Very Large Datasets** (> 1GB): Configurable chunking

---

## ğŸ”® **Roadmap & Future Features**

### ğŸ¯ **Short Term** (Next 3 months)
- [ ] **Database Support**: Direct database connections
- [ ] **API Endpoints**: RESTful API for integration
- [ ] **Real-time Processing**: Streaming data support
- [ ] **Enhanced Visualizations**: Interactive charts and dashboards

### ğŸš€ **Medium Term** (3-6 months)
- [ ] **Machine Learning Pipeline**: Automated ML model training
- [ ] **Cloud Integration**: AWS, Azure, GCP support
- [ ] **Collaborative Features**: Team workspaces and sharing
- [ ] **Advanced Rules**: Custom Python rule functions

### ğŸŒŸ **Long Term** (6+ months)
- [ ] **AI-Powered Cleaning**: Intelligent data transformation suggestions
- [ ] **Multi-language Support**: R, Julia, and other language bindings
- [ ] **Enterprise Features**: Role-based access control and audit trails
- [ ] **Mobile App**: iOS and Android applications

---

## ğŸ¤ **Contributing**

We welcome contributions! See [CONTRIBUTING.md](./CONTRIBUTING.md) for:
- ğŸš€ **Setup Guide**: Local development environment
- ğŸ“ **Code Style**: Python and project standards
- ğŸ§ª **Testing**: Writing and running tests
- ğŸ”„ **PR Process**: Pull request guidelines
- ğŸ› **Bug Reports**: Issue reporting templates

---

## ğŸ“„ **License**

This project is licensed under the MIT License - see the [LICENSE](./LICENSE) file for details.

---

## ğŸ™ **Acknowledgments**

- **pandas**: Data manipulation and analysis
- **scikit-learn**: Machine learning algorithms
- **matplotlib/seaborn**: Data visualization
- **Typer/Rich**: Beautiful CLI interfaces
- **Streamlit**: Web application framework

---

<div align="center">

**Made with â¤ï¸ by the CleanEngine Community**

[![GitHub stars](https://img.shields.io/github/stars/I-invincib1e/CleanEngine?style=social)](https://github.com/I-invincib1e/CleanEngine)
[![GitHub forks](https://img.shields.io/github/forks/I-invincib1e/CleanEngine?style=social)](https://github.com/I-invincib1e/CleanEngine)
[![GitHub issues](https://img.shields.io/github/issues/I-invincib1e/CleanEngine)](https://github.com/I-invincib1e/CleanEngine/issues)

**â­ Star this repository if it helped you! â­**

</div>
